\documentclass[12pt]{article}
\bibliographystyle{plain}
%
\usepackage{amsmath}
%\usepackage{geometry}
%\usepackage{algpseudocode}
%\usepackage{algorithm}
%
%
%\geometry{letterpaper,
%          left   = 1in,
%          right  = 1in,
%          top    = 1in,
%          bottom = 1in}
%\renewcommand{\baselinestretch}{1.1}
%\renewcommand{\arraystretch}{1.1}
%\usepackage{amsmath, amsthm, amssymb} % Latex Math Include
%
%
%% Custom commands
%% ----------------------------------------------------------------------------------------------------
%\newcounter{count}
%\newcommand{\bS}[1] {\boldsymbol  #1}
%\newcommand{\aI}{\forall i \in \{1,2,\ldots, N\}}
%\newcommand{\aJ}{\forall j \in \{1,2,\ldots, J\}}
%\newcommand{\MLE}{\hat{\bS{\beta}}_{MLE}}

% Title
% ----------------------------------------------------------------------------------------------------

%\author{Mark Wellons}
\title{Robust Variance via Huber-White Sandwich Estimators}
\begin{document}
\maketitle
%\large
%\begin{center}
%{\sc \bf Design Document -- Not for Distribution}
%\end{center}
%\normalsize

\section{Introduction}
Given  $n$ data points, each defined by a feature  vector $x_i$ and a category $y_i$, we assume that $y_i$ is drawn from a distribution determined by a $k$-dimensional parameter vector $\theta$.  Generally, we are interested in finding the values of $\theta$ that best predict $y_i$ from $x_i$, with \textit{best} being defined as the values that maximize some likelihood function $L(y,x,\theta)$.  The maximization is typically solved using the derivative of the likelihood $\psi$  and the Hessian $H$.  More formally, $\psi$ is defined as 
\begin{align}
\psi(y,x, \theta) = \frac{\partial L(x,y,\theta)}{\partial \theta}
\end{align} 
and $H$ is defined as
\begin{align}
H(y,x, \theta) = \frac{\partial^2 L(x,y,\theta)}{\partial \theta^2}.
\end{align} 
Using these derivatives, one can solve the logistic or linear regression for the optimal solution, and compute the variance and other statistics of the regression.  


However, we may believe that the underlying model is incorrect (our assumption that  $y_i$ is determined by $\theta$ is wrong), in which case, our variance estimates will be incorrect.  The Huber sandwich estimator is used to get a estimate of the variance robust to specification errors in the underlying model.  The estimator is known as a sandwich estimator because the robust covariance matrix $S(\theta)$ of $\theta$ can be expressed in a \textit{sandwich formulation}, of the form
\begin{align}
S(\theta) = B(\theta) M(\theta) B(\theta).  
\end{align}
The $B(\theta)$ matrix is commonly called the \textit{bread}, whereas the $M(\theta)$ matrix is the \textit{meat}.  

\subsection{The Bread}
Computing $B$ is relatively straightforward, 
\begin{align}
B(\theta) = n\left(\sum_i^n -H(y_i, x_i, \theta) \right)^{-1}
\end{align}

\subsection{The Meat}
There are several choices for the $M$ matrix, each with different robustness properties.  In the Huber-White estimator, the matrix $M$ is defined as
\begin{align}
M_{H} =\frac{1}{n} \sum_i^n \psi(y_i,x_i, \theta)^T  \psi(y_i,x_i, \theta).
\end{align}

%The Huber/White estimator is a diagonal matrix defined as
%\begin{align}
%M_W =\frac{1}{n} X^T \left(
%      \begin{array}{cccc}
%        r(y_1, x_1^T\theta) & 0&\ldots &0 \\
%        0&r(y_2, x_2^T\theta) &\dots& \vdots \\
%	\vdots & \vdots & \ddots&\vdots \\
%        0 & \ldots & \ldots &r(y_n, x_n^T\theta)
%      \end{array} \right) X.
%\end{align}
%The matrix $X$ is the data matrix, and $r$ is the residual function.  The residual function is defined by the application, but it has the relationship
%\begin{align}
%  \psi(y_i,x_i, \theta) = r(y_i, x_i^T\theta)x_i.  
%\end{align} 
%This means that it can be computed in the general case as 
%\begin{align}
% r(y_i, x_i^T\theta) =  \frac{\psi(y_i,x_i, \theta)}{x_i}
%\end{align}
%


\section{Implementation}

The Huber-White sandwich estimators implemented for linear, logistic, and multinomial-logistic regression mimic the same framework as  the linear/logistic regression implementations.  In these implementations, the gradient and Hessian are computed in parallel, and a final step operates on the aggregate result.  

This framework breaks the computation into three steps: \textit{transition} step, the \textit{merge states} step, and the \textit{final} step.  The transition step computes the gradient and Hessian contribution from each row in the data matrix. The merge step sum two or more steps together.  The final step computes the inverse of the Hessian, the coefficients of the sandwich operator, and other relevant statistics appropriate to the regression.  %The gradient and Hessian calculations are identical to the gradient and Hessian calculations for logistic regression.  

We note that the Hessian and derivative both depend on $\theta$, so the variance estimates will also depend on $\theta$.  Rather than allow the user to specify a $\theta$ value, the implementation computes the optimal $\theta$ by running the appropriate regression  before computing the robust variance.  In cases where the regression has parameters (regression tolerance, max iterations), the interface allows the user to specify those parameters.


\section{Input}
	The robust variance function calls all have similar interfaces.  The common argument parameters are (in order):
\begin{description}
\item [source table] The table containing the source data.  
\item [output table] The name of the table containg the computed statistics.  The function throws an error if the output table already exists. 
\item [dependent variable] The dependent variable in the regression.   
\item [independent variables]  The independent variables in the regression.    
\item [grouping variable(s)]  The source table is divided into disjoint groups by the value of the grouping variable.  The robust variance and other statistics are computed and returned for each group.  
\end{description}
	In addition, each robust variance calculation can have arguments specific to that regression.  Regression specific arguments  always come after the common arguments.  

\section{Output}
	The output will be written to the table specified in the function call, and includes:
\begin{description}
\item [regression coefficients] The optimal coefficients computed by the regression.  
\item [standard errors] The standard errors of the robust variance.
\item [t- or z-statistics] The t- or z-statistics of the robust variance.   
\item [p-values]  The p-values.      
\end{description}

	


\end{document}


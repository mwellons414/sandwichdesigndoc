\documentclass[12pt]{article}
\bibliographystyle{plain}
%
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{geometry}
%\usepackage{algpseudocode}
%\usepackage{algorithm}
%
%
%\geometry{letterpaper,
%          left   = 1in,
%          right  = 1in,
%          top    = 1in,
%          bottom = 1in}
%\renewcommand{\baselinestretch}{1.1}
%\renewcommand{\arraystretch}{1.1}
%\usepackage{amsmath, amsthm, amssymb} % Latex Math Include
%
%
%% Custom commands
%% ----------------------------------------------------------------------------------------------------
%\newcounter{count}
%\newcommand{\bS}[1] {\boldsymbol  #1}
%\newcommand{\aI}{\forall i \in \{1,2,\ldots, N\}}
%\newcommand{\aJ}{\forall j \in \{1,2,\ldots, J\}}
%\newcommand{\MLE}{\hat{\bS{\beta}}_{MLE}}

% Title
% ----------------------------------------------------------------------------------------------------

%\author{Mark Wellons}
\title{Robust Variance via Huber-White Sandwich Estimators}
\begin{document}
\maketitle
%\large
%\begin{center}
%{\sc \bf Design Document -- Not for Distribution}
%\end{center}
%\normalsize

\section{Introduction}
Given  $n$ data points, where the $i$th point is defined by a feature  vector $x_i \in \mathbb{R}^m$ and a category scalar $y_i$, where $y_i \in \mathbb{R}, y_i \in \{0,1 \}$, and $y_i \in \{1,\dots, l \}$ for linear, logistic, and multi-logistic regression respectively.  We assume that $y_i$ is drawn from an independent and identically distributed (i.i.d.) distribution determined by a $g$-dimensional parameter vector $\beta$ (which is $g\times l$ dimensional for multi-logistic regression).  

Generally, we are interested in finding the values of $\beta$ that best predict $y_i$ from $x_i$, with \textit{best} being defined as the values that maximize some log-likelihood function $L(y,x,\beta)$.  The maximization is typically solved using the derivative of the likelihood $\psi$  and the Hessian $H$.  More formally, $\psi$ is defined as 
\begin{align}
\psi(y,x, \beta) = \frac{\partial L(x,y,\beta)}{\partial \beta}
\end{align} 
and $H$ is defined as
\begin{align}
H(y,x, \beta) = \frac{\partial^2 L(x,y,\beta)}{\partial \beta^2}.
\end{align} 
Using these derivatives, one can solve the logistic or linear regression for the optimal solution, and compute the variance and other statistics of the regression.  


However, we may believe that the underlying distribution is not i.i.d., (in particular, the variance is not independent of $x_i$), in which case, our variance estimates will be incorrect. 
 The Huber sandwich estimator is used to get a robust estimate of the variance even if the iid assumption is wrong.  The estimator is known as a sandwich estimator because the robust covariance matrix $S(\beta)$ of $\beta$ can be expressed in a \textit{sandwich formulation}, of the form
\begin{align}
S(\beta) = B(\beta) M(\beta) B(\beta).  
\end{align}
The $B(\beta)$ matrix is commonly called the \textit{bread}, whereas the $M(\beta)$ matrix is the \textit{meat}.  

\subsection{The Bread}
Computing $B$ is relatively straightforward, 
\begin{align}
B(\beta) = n\left(\sum_i^n -H(y_i, x_i, \beta) \right)^{-1}
\end{align}

\subsection{The Meat}
There are several choices for the $M$ matrix, each with different robustness properties.  In the Huber-White estimator, the matrix $M$ is defined as
\begin{align}
M_{H} =\frac{1}{n} \sum_i^n \psi(y_i,x_i, \beta)^T  \psi(y_i,x_i, \beta).
\end{align}

%The Huber/White estimator is a diagonal matrix defined as
%\begin{align}
%M_W =\frac{1}{n} X^T \left(
%      \begin{array}{cccc}
%        r(y_1, x_1^T\beta) & 0&\ldots &0 \\
%        0&r(y_2, x_2^T\beta) &\dots& \vdots \\
%	\vdots & \vdots & \ddots&\vdots \\
%        0 & \ldots & \ldots &r(y_n, x_n^T\beta)
%      \end{array} \right) X.
%\end{align}
%The matrix $X$ is the data matrix, and $r$ is the residual function.  The residual function is defined by the application, but it has the relationship
%\begin{align}
%  \psi(y_i,x_i, \beta) = r(y_i, x_i^T\beta)x_i.  
%\end{align} 
%This means that it can be computed in the general case as 
%\begin{align}
% r(y_i, x_i^T\beta) =  \frac{\psi(y_i,x_i, \beta)}{x_i}
%\end{align}
%


\section{Implementation}

The Huber-White sandwich estimators implemented for linear, logistic, and multinomial-logistic regression mimic the same framework as  the linear/logistic regression implementations.  In these implementations, the gradient and Hessian are computed in parallel, and a final step operates on the aggregate result.  

This framework breaks the computation into three steps: \textit{transition} step, the \textit{merge states} step, and the \textit{final} step.  The transition step computes the gradient and Hessian contribution from each row in the data matrix.  To compute this step, we need to define the derivatives for each regression.  
\subsection{Linear Regression Derivatives}
For linear regression, the derivative is 
\begin{align}
\frac{\partial L(x,y,\beta)}{\partial \beta} = X^T(y - X \beta)
\end{align}
and the Hessian is
\begin{align}
\frac{\partial^2 L(x,y,\beta)}{\partial^2 \beta} = -X^TX. 
\end{align}

\subsection{Logistic Regression Derivatives}
For logistic, the derivative is 
\begin{align}
\frac{\partial L(x,y,\beta)}{\partial \beta} = \sum_i \frac{-1^{y_i}\cdot \beta}{1 + \exp(-1^{y_i} \cdot  \beta^Tx)}  \exp(-1^{y_i}\cdot \beta^Tx)
\end{align}
and the Hessian is
\begin{align}
\frac{\partial^2 L(x,y,\beta)}{\partial^2 \beta} = -X^TAX = -\sum_i A_{ii} x_i^T x_i 
\end{align}
where $A$ is a diagonal matrix with 
\begin{align}
A_{ii} = \left( [1 + \exp(c^Tx_i) ) (1 + \exp(-c^Tx_i)] \right)^{-1}.
\end{align}

\subsection{Multi-Logistic Regression Derivatives}
For multi-logistic regression,  we replace $y_i$ with a vector $Y_i \in \{0,1\}^l$, where all entries of $Y_i$ are zero expect the $y_i$th entry, which is set to 1.  In addition, we choose a \textit{baseline} category, for which the odds of all the categories are measured against.  Let $J \in \{1, \dots, l \}$ be the baseline category.  

Then we define the variables
\begin{align}
\pi_{i,j} &= \frac{\exp\left(\sum_{k=1}^g x_{i,k} \beta_{k,j} \right)}{1 + \sum_{j\ne J} \exp \left(\sum_{k=1}^g x_{i,k} \beta_{k,j} \right)}, \ \ \forall j \ne J\\
\pi_{i,J} &= \frac{1}{1 + \sum_{j\ne J} \exp \left(\sum_{k=1}^k x_{i,k} \beta_{k,j} \right)}.
\end{align}

The derivatives are then
\begin{equation}\label{eq:first_derivative}
\frac{\partial L}{\partial \beta_{k,j}} = \sum_{i=1}^{N} y_{i,j}x_{i,k} - \pi_{i,j}x_{i,k} \ \ \ \ \forall k \  \forall j
\end{equation}
The Hessian is then 
\begin{align}\label{eq:second_derivative}
\frac{\partial^2 l({\beta})}{\partial \beta_{k_2,j_2} \partial \beta_{k_1,j_1}} 
&= \sum_{i=1}^{N} -\pi_{i,j_2}x_{i,k_2}(1-\pi_{i,j_1})x_{i,k_1} &&j_1 = j_2 \\
&= \sum_{i=1}^{N} \pi_{i,j_2}x_{i,k_2}\pi_{i,j_1}x_{i,k_1} &&j_1 \neq j_2 
\end{align}

\subsection{Merge Step and Final Step}

For the logistic and multi-logistic, the derivative and Hessian are sums in which the terms can be computed in parallel, and thus in a distributed manner.  Each transition step computes a single term in the sum.  The merge step sums two or more terms computed by the transition steps, and the final step computes the Hessian inverse, and the matrix product between the bread and meat matrices.  


We note that the Hessian and derivative both depend on $\beta$, so the variance estimates will also depend on $\beta$.  Rather than allow the user to specify a $\beta$ value, the implementation computes the optimal $\beta$ by running the appropriate regression  before computing the robust variance.  In cases where the regression has parameters (regression tolerance, max iterations), the interface allows the user to specify those parameters.

	


\end{document}

